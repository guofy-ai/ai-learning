{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f37ace-1136-4997-8ee7-2391b64b1fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPT 模型（Open Pre-trained Transformer）是 Meta 开源的大型语言模型，其核心是 Next Token Prediction（下一个词预测）任务。\n",
    "#作用：\n",
    "#  根据上下文预测下一个最可能的词/标记（token）\n",
    "#  用于文本生成（续写故事、对话、代码等）\n",
    "#  学习语言统计规律和语义关系\n",
    "#Next Token Prediction 原理：\n",
    "#  输入文本被拆分为 tokens（如：\"Hello\" → [\"He\", \"llo\"]）\n",
    "#  模型通过自注意力机制计算上下文表示\n",
    "#  输出层预测下一个 token 的概率分布\n",
    "#  选择概率最高的 token（或按概率采样）作为预测结果\n",
    "\n",
    "#export HF_ENDPOINT=https://hf-mirror.com\n",
    "#huggingface-cli download facebook/opt-125m --local-dir /mnt/workspace/ai/models/opt-125m\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# 1. 加载 OPT 模型和分词器（使用小规模版本节省资源）\n",
    "model_name = \"/mnt/workspace/ai/models/opt-125m\"  # 可选：opt-350m, opt-1.3b 等\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86eb640-9cfc-47f7-b4b5-474c37f1fc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入文本: 'The future of artificial intelligence'\n",
      "\n",
      "预测的下一个 token 及概率:\n",
      "1.  is        | 概率: 0.2814\n",
      "2. \n",
      "          | 概率: 0.1943\n",
      "3.  and       | 概率: 0.0496\n",
      "4.  (         | 概率: 0.0469\n",
      "5.  will      | 概率: 0.0403\n",
      "\n",
      "生成完整续写:\n",
      "The future of artificial intelligence\n",
      "\n",
      "The future of artificial intelligence is a subject that is constantly on the mind of many many people\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. 输入文本\n",
    "text = \"The future of artificial intelligence\"\n",
    "\n",
    "# 3. 编码输入 → 生成 token IDs\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")  # 格式: torch tensor\n",
    "\n",
    "# 4. 预测下一个 token (贪婪搜索)\n",
    "with torch.no_grad():  #推理禁用梯度\n",
    "    outputs = model(input_ids)\n",
    "    next_token_logits = outputs.logits[:, -1, :]  # 取最后一个位置的 logits\n",
    "    \n",
    "# 5. 计算概率分布\n",
    "probs = torch.softmax(next_token_logits, dim=-1) #激活函数\n",
    "topk_probs, topk_tokens = torch.topk(probs, 5, dim=-1)  # 取概率最高的 5 个 token\n",
    "\n",
    "# 6. 解码并打印结果\n",
    "print(f\"输入文本: '{text}'\\n\")\n",
    "print(\"预测的下一个 token 及概率:\")\n",
    "for i in range(5):\n",
    "    token = tokenizer.decode(topk_tokens[0][i])\n",
    "    prob = topk_probs[0][i].item()\n",
    "    print(f\"{i+1}. {token:<10} | 概率: {prob:.4f}\")\n",
    "\n",
    "# 7. 完整文本生成（续写 20 个 token）\n",
    "generated = model.generate(\n",
    "    input_ids,\n",
    "    max_length=len(input_ids[0]) + 20,  # 续写长度\n",
    "    do_sample=True,          # 启用采样（非贪婪）\n",
    "    top_k=50,                # 从高概率的 50 个 token 中采样\n",
    "    temperature=0.7,         # 控制随机性（0=确定，1=随机）\n",
    ")\n",
    "print(\"\\n生成完整续写:\")\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b18ce-077a-4853-800a-0681982a2dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI开发",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
